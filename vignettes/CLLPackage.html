<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>CLLPackage</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">CLLPackage</h1>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CLLPackage)</span></code></pre></div>
<p>The purpose of this package is to provide functionalities to estimate the regression coefficients of the linear model <span class="math display">\[Y = X\beta+\varepsilon\]</span> where <span class="math inline">\(X\)</span> is the design matrix, <span class="math inline">\(Y\)</span> a response variable, <span class="math inline">\(\beta\)</span> a coefficients vector and <span class="math inline">\(\varepsilon\)</span> a normal random noise. The estimation is performed by minimizing the loss function <span class="math display">\[L(\beta)=||X\beta-Y||_2^2\]</span></p>
<p>Two different iterative minimization methods are available in the package, the <span class="math inline">\(\textit{gradient descend}\)</span> and the <span class="math inline">\(\textit{steepest descend}\)</span> methods. Moreover, a <span class="math inline">\(k\)</span>-fold cross-validation procedure is provided, using the MSE (Mean Squared Error) as performance index. Sequential and parallel versions are implemented for this task.</p>
<div id="gradient-descend-method" class="section level2">
<h2>Gradient descend method</h2>
<p>The gradient descend algorithm proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The initial point <span class="math inline">\(\hat{\beta}(0)\)</span> is chosen.</p></li>
<li><p>For <span class="math inline">\(t=0,1,\ldots\)</span>, the new iterate <span class="math inline">\(\hat{\beta}(t+1)\)</span> is obtained by the rule: <span class="math display">\[\hat{\beta}(t+1) = \hat{\beta}(t) -\gamma \nabla L(\hat{\beta}(t))
  \]</span> where <span class="math inline">\(\gamma&gt;0\)</span> is the <span class="math inline">\(\textit{stepsize}\)</span> (chosen by the user).</p></li>
<li><p>Stop the procedure when the stopping conditions are satisfied. In the package, two criteria are implemented: the maximum number of iterations <span class="math inline">\(\textit{maxit}\)</span> and a condition on non-significant improvements of the estimate, i.e. <span class="math inline">\(\max_i|\hat{\beta}_i(t+1) - \hat{\beta}_i(t)|&lt; tol\)</span>, where <span class="math inline">\(\textit{tol}\)</span> is a pre-specified tolerance.</p></li>
</ol>
<p>The following is an example of the use of the function <span class="math inline">\(\texttt{ linear_gd_optim()}\)</span>. Note that the performance of the algorithm strongly depends on the choice of the parameters <span class="math inline">\(\textit{tol, maxit}, \gamma\)</span>. In this example, the parameters are chosen properly. Later, some considerations on this issue are discussed. Comparing the estimates obtained with <span class="math inline">\(\textit{gradient descend}\)</span> method and the ones computed by <span class="math inline">\(\texttt{lm()}\)</span> R function, it is clear that estimates are quite similar.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8675309</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">10000</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x1<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x2<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x1,x2)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with gradient descend method</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>(beta_estimated_gd <span class="ot">&lt;-</span> <span class="fu">linear_gd_optim</span>(beta0, X, y, <span class="at">maxit =</span> <span class="dv">1000</span>, <span class="at">stepsize =</span> <span class="fl">1e-5</span>, <span class="at">tol=</span><span class="fl">1e-5</span>))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Error less than 1e-05 after 49 iterations&quot;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.0110419 0.4816794 0.2107610</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with OLS</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>(<span class="at">beta_min =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)<span class="sc">$</span>coeff)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)          x1          x2 </span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1.0110145   0.4816491   0.2107263</span></span></code></pre></div>
</div>
<div id="steepest-descend-method" class="section level2">
<h2>Steepest descend method</h2>
<p>The steepest descend algorithm proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The initial point <span class="math inline">\(\hat{\beta}(0)\)</span> is chosen.</p></li>
<li><p>For <span class="math inline">\(t=0,1,\ldots\)</span>, the new iterate <span class="math inline">\(\hat{\beta}(t+1)\)</span> is obtained by the rule: <span class="math display">\[\hat{\beta}(t+1) = \hat{\beta}(t) -\text{step}(t) \cdot \nabla L(\hat{\beta}(t))
  \]</span> where, denoting with <span class="math inline">\(H(\hat{\beta}(t)) = 4X^T X\)</span> the Hessian of <span class="math inline">\(L\)</span>, <span class="math display">\[\text{step}(t) = \frac{||\nabla L(\hat{\beta}(t))||^2}{\nabla L(\hat{\beta}(t))^T\, H(\hat{\beta}(t)) \,\nabla L(\hat{\beta}(t))}
  \]</span></p></li>
<li><p>Stop the procedure when the stopping conditions are satisfied. Similarly to the gradient descend algorithm, two criteria are implemented: the maximum number of iterations <span class="math inline">\(\textit{maxit}\)</span> and a condition on non-significant improvements of the estimate, i.e. <span class="math inline">\(\max_i|\hat{\beta}_i(t+1) - \hat{\beta}_i(t)|&lt; tol\)</span>, where <span class="math inline">\(\textit{tol}\)</span> is a pre-specified tolerance.</p></li>
</ol>
<p>Differently from the gradient descend method, where the <span class="math inline">\(\textit{stepsize}\)</span> was chosen by the user and fixed during the whole procedure, here the stepsize <span class="math inline">\(\text{step}(t)\)</span> is automatically computed and varies along the iterations.</p>
<p>The following is an example of the use of the function <span class="math inline">\(\texttt{ linear_sd_optim()}\)</span>. Comparing the estimates obtained with <span class="math inline">\(\textit{steepest descend}\)</span> method and the ones computed by <span class="math inline">\(\texttt{lm()}\)</span> R function, it is clear that estimates are quite similar.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with gradient descend method</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>(beta_estimated_sd <span class="ot">&lt;-</span> <span class="fu">linear_sd_optim</span>(beta0, X, y, <span class="at">maxit =</span> <span class="dv">1000</span>, <span class="at">tol=</span><span class="fl">1e-4</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Error less than 1e-04 after 24 iterations&quot;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.0111904 0.4822329 0.2110153</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with OLS</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>(beta_min <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)<span class="sc">$</span>coeff)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)          x1          x2 </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1.0110145   0.4816491   0.2107263</span></span></code></pre></div>
<div id="comparison-between-gradient-and-steepest-descend-methods" class="section level3">
<h3>Comparison between gradient and steepest descend methods</h3>
<p>As already pointed out, the difference between the two methods consists of the <span class="math inline">\(\textit{stepsize}\)</span> choice, which modulates step length along the direction identified by the gradient: in the <span class="math inline">\(\textit{gradient descend}\)</span> algorithm it is chosen by the user and remains fixed for the whole iterative process, while in the <span class="math inline">\(\textit{steepest descend}\)</span> methods it is automatically computed by the algorithm based on the current estimate of <span class="math inline">\(\beta\)</span>, thus it should better exploit the geometry of the problem.</p>
<p>Two main issues can be encountered performing the <span class="math inline">\(\textit{gradient descend}\)</span> method:</p>
<ol style="list-style-type: decimal">
<li>if the pre-specified <span class="math inline">\(\textit{stepsize}\)</span>, <span class="math inline">\(\gamma\)</span>, is too large for the problem, the algorithm might never converge to the minimum, since it keeps bouncing around the optimal solution and possibly diverges from it. In the following example, the error does not decrease along the iterations, even reaching <span class="math inline">\(Inf\)</span> value.</li>
</ol>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with gradient descend method</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>beta_estimated_gd <span class="ot">&lt;-</span> <span class="fu">linear_gd_optim</span>(beta0, X, y, <span class="at">maxit =</span> <span class="dv">10000</span>, <span class="at">stepsize =</span> <span class="fl">1e-2</span>, <span class="at">tol=</span><span class="fl">1e-2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Infinite error after 133 iterations&quot;</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>for good performance of the algorithm, it is important to choose values for <span class="math inline">\(stepsize\)</span> and <span class="math inline">\(tolerance\)</span> with the same order of magnitude. Otherwise, if <span class="math inline">\(stepsize\)</span> is too small with respect to <span class="math inline">\(tolerance\)</span>, the algorithm might stop too early having no chance to reach the solution to the minimization problem. In the following example, comparing estimates provided by <span class="math inline">\(\texttt{linear_gd_optim()}\)</span> and the ones provided by <span class="math inline">\(\texttt{lm()}\)</span>, it is clear that the <span class="math inline">\(\textit{gradient descend}\)</span> method has not reached the minimum point.</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),x1,x2)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with gradient descend method</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>(beta_estimated_gd <span class="ot">&lt;-</span> <span class="fu">linear_gd_optim</span>(beta0, X, y, <span class="at">maxit =</span> <span class="dv">10000</span>, <span class="at">stepsize =</span> <span class="fl">1e-6</span>, <span class="at">tol=</span><span class="fl">1e-2</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;Error less than 0.01 after 36 iterations&quot;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5208087 0.2510554 0.1015211</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># beta estimated with OLS</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>(<span class="at">beta_min =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)<span class="sc">$</span>coeff)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)          x1          x2 </span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1.0110145   0.4816491   0.2107263</span></span></code></pre></div>
<p>The <span class="math inline">\(\textit{steepest descend}\)</span> method solves the two issues by computing the <span class="math inline">\(stepsize\)</span> for each iteration, taking into account the previous estimate of <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p>The package includes a sequential (<span class="math inline">\(\texttt{kfold_cv_seq()}\)</span>) and a parallel (<span class="math inline">\(\texttt{kfold_cv_parallel()}\)</span>) implementation of the k-fold cross-validation procedure, which returns an estimate of the Mean Squared Error (MSE) of predictions based on the linear model, where <span class="math inline">\(\beta\)</span> is estimated via steepest or gradient descend method, according to the choice specified by the user (default option is steepest descend).</p>
<p>In the following example, the MSE is computed via <span class="math inline">\(\texttt{kfold_cv_seq()}\)</span>, <span class="math inline">\(\texttt{kfold_cv_parallel()}\)</span> and <span class="math inline">\(\texttt{kfold_cv_lm_seq()}\)</span>, which is a function that returns MSE computed performing k-fold cross-validation using R function <span class="math inline">\(\texttt{lm()}\)</span>. It can be observed that they are similar. Moreover, for values for a sufficiently large value of <span class="math inline">\(k\)</span>, which is the number of folds, the execution time of <span class="math inline">\(\texttt{kfold_cv_seq()}\)</span> is longer than <span class="math inline">\(\texttt{kfold_cv_parallel()}\)</span>, as expected. In fact, the parallel version starts to gain in terms of execution time when fixed costs related to cluster creation and splitting of the jobs to the workers are compensated.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Sequential CV MSE</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>(MSE_seq <span class="ot">&lt;-</span> <span class="fu">kfold_cv_seq</span>(X,y,<span class="at">k=</span><span class="dv">1000</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.010367</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 43.3 sec elapsed</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Parallel CV MSE</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>(MSE_parallel <span class="ot">&lt;-</span> <span class="fu">kfold_cv_parallel</span>(X,y,<span class="at">k=</span><span class="dv">1000</span>))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.010348</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 13.84 sec elapsed</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># lm MSE</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>(MSE <span class="ot">&lt;-</span> <span class="fu">kfold_cv_lm_seq</span>(<span class="at">X =</span> X, <span class="at">y=</span>y, <span class="at">k=</span><span class="dv">1000</span>))</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.010284</span></span></code></pre></div>
<p>To implement the parallel version, library <span class="math inline">\(\texttt{doSNOW}\)</span> has been used and map-reduce paradigm has been adopted: <span class="math inline">\(map\)</span> function includes the creation of the training and the test sets, the estimation of the <span class="math inline">\(\beta\)</span> on the training set, the predictions of the responses on the test set and, finally, the computation of the MSE. <span class="math inline">\(Reduce\)</span> function computes the average of the <span class="math inline">\(k\)</span> values of the MSE obtained during <span class="math inline">\(map\)</span> process.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
